{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjObkd+zVF7UL2sNTTVOEV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunnyZhao2004/Data_Project/blob/main/BiLSTM_CRF00.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive;\n",
        "drive.mount('/content/drive');\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CklFTWFLQ1jv",
        "outputId": "4dc6fc94-a4cc-4f14-93ef-b4d79086425f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import ast\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Define the mapping and helper functions\n",
        "# ---------------------------\n",
        "\n",
        "label_to_id = {\n",
        "    \"B-art\": 0,\n",
        "    \"B-eve\": 1,\n",
        "    \"B-geo\": 2,\n",
        "    \"B-gpe\": 3,\n",
        "    \"B-nat\": 4,\n",
        "    \"B-org\": 5,\n",
        "    \"B-per\": 6,\n",
        "    \"B-tim\": 7,\n",
        "    \"I-art\": 8,\n",
        "    \"I-eve\": 9,\n",
        "    \"I-geo\": 10,\n",
        "    \"I-gpe\": 11,\n",
        "    \"I-nat\": 12,\n",
        "    \"I-org\": 13,\n",
        "    \"I-per\": 14,\n",
        "    \"I-tim\": 15,\n",
        "    \"O\": 16\n",
        "}\n",
        "\n",
        "def process_label(label):\n",
        "    \"\"\"\n",
        "    Given a label stored as a byte-string (e.g.,\n",
        "    b\"['O', 'O', 'O', ...]\"),\n",
        "    decode it (if necessary) and convert the string representation\n",
        "    into a Python list.\n",
        "    \"\"\"\n",
        "    if isinstance(label, bytes):\n",
        "        label = label.decode('utf-8')\n",
        "    # Use ast.literal_eval to safely convert string to list.\n",
        "    return ast.literal_eval(label)\n",
        "\n",
        "def map_labels_to_ids(label_list, mapping):\n",
        "    \"\"\"\n",
        "    Map each label in label_list to its numeric ID using the provided mapping.\n",
        "    \"\"\"\n",
        "    return [mapping[label] for label in label_list]\n",
        "\n",
        "def pad_labels(label_list, max_length):\n",
        "    \"\"\"\n",
        "    Pads a list of label IDs with 0s until its length equals max_length.\n",
        "    (Note: Ensure that using 0 as pad does not conflict with a valid label.)\n",
        "    \"\"\"\n",
        "    return label_list + [0] * (max_length - len(label_list))\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Load and split the dataset\n",
        "# ---------------------------\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/comp-4211-spring-25-project/\"\n",
        "csv_path = folder_path + \"train.csv\"\n",
        "\n",
        "# Read the CSV file.\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Manually split the dataset:\n",
        "train_dataset_df = df[:32000].reset_index(drop=True)\n",
        "test_dataset_df = df[32000:].reset_index(drop=True)\n",
        "\n",
        "print(\"Train dataset shape:\", train_dataset_df.shape)\n",
        "print(\"Test dataset shape:\", test_dataset_df.shape)\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Process the NER tag column for both train and test sets\n",
        "# ---------------------------\n",
        "\n",
        "# Process train set:\n",
        "train_dataset_df['processed_NER_tag'] = train_dataset_df['NER Tag'].apply(process_label)\n",
        "train_dataset_df['NER_tag_ids'] = train_dataset_df['processed_NER_tag'].apply(lambda x: map_labels_to_ids(x, label_to_id))\n",
        "# Determine the maximum sequence length from the training set.\n",
        "max_length = train_dataset_df['NER_tag_ids'].apply(len).max()\n",
        "print(\"Maximum label sequence length (train):\", max_length)\n",
        "# Pad the sequences:\n",
        "train_dataset_df['NER_tag_ids_padded'] = train_dataset_df['NER_tag_ids'].apply(lambda x: pad_labels(x, max_length))\n",
        "\n",
        "# Process test set:\n",
        "test_dataset_df['processed_NER_tag'] = test_dataset_df['NER Tag'].apply(process_label)\n",
        "test_dataset_df['NER_tag_ids'] = test_dataset_df['processed_NER_tag'].apply(lambda x: map_labels_to_ids(x, label_to_id))\n",
        "test_dataset_df['NER_tag_ids_padded'] = test_dataset_df['NER_tag_ids'].apply(lambda x: pad_labels(x, max_length))\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Convert to NumPy arrays and create tf.data.Datasets\n",
        "# ---------------------------\n",
        "\n",
        "# Assume the text sentence column is named \"sentence\"\n",
        "train_sentences = train_dataset_df['Sentence'].values  # shape: (num_train_examples,)\n",
        "test_sentences = test_dataset_df['Sentence'].values    # shape: (num_test_examples,)\n",
        "\n",
        "# Convert padded label lists to a NumPy array of shape (num_examples, max_length)\n",
        "train_labels = np.array(train_dataset_df['NER_tag_ids_padded'].tolist(), dtype=np.int32)\n",
        "test_labels = np.array(test_dataset_df['NER_tag_ids_padded'].tolist(), dtype=np.int32)\n",
        "\n",
        "print(\"Train sentences array shape:\", train_sentences.shape)\n",
        "print(\"Train labels array shape:\", train_labels.shape)\n",
        "\n",
        "# Create TensorFlow datasets from the numpy arrays.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels))\n",
        "\n",
        "# Optionally, you can batch and prefetch your datasets:\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# ---------------------------\n",
        "# 5. (Optional) Preview a few examples from the tf.data.Dataset\n",
        "# ---------------------------\n",
        "for sentence, labels in train_dataset.take(1):\n",
        "    print(\"Sentence:\", sentence.numpy())\n",
        "    print(\"Padded NER tag IDs:\", labels.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3E7qjCcPzMb",
        "outputId": "363bc579-7ee0-4194-9de9-1002ebfd936a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset shape: (32000, 3)\n",
            "Test dataset shape: (8000, 3)\n",
            "Maximum label sequence length (train): 104\n",
            "Train sentences array shape: (32000,)\n",
            "Train labels array shape: (32000, 104)\n",
            "Sentence: [b\"['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.']\"\n",
            " b\"['Iranian', 'officials', 'say', 'they', 'expect', 'to', 'get', 'access', 'to', 'sealed', 'sensitive', 'parts', 'of', 'the', 'plant', 'Wednesday', ',', 'after', 'an', 'IAEA', 'surveillance', 'system', 'begins', 'functioning', '.']\"\n",
            " b\"['Helicopter', 'gunships', 'Saturday', 'pounded', 'militant', 'hideouts', 'in', 'the', 'Orakzai', 'tribal', 'region', ',', 'where', 'many', 'Taliban', 'militants', 'are', 'believed', 'to', 'have', 'fled', 'to', 'avoid', 'an', 'earlier', 'military', 'offensive', 'in', 'nearby', 'South', 'Waziristan', '.']\"\n",
            " b\"['They', 'left', 'after', 'a', 'tense', 'hour-long', 'standoff', 'with', 'riot', 'police', '.']\"\n",
            " b\"['U.N.', 'relief', 'coordinator', 'Jan', 'Egeland', 'said', 'Sunday', ',', 'U.S.', ',', 'Indonesian', 'and', 'Australian', 'military', 'helicopters', 'are', 'ferrying', 'out', 'food', 'and', 'supplies', 'to', 'remote', 'areas', 'of', 'western', 'Aceh', 'province', 'that', 'ground', 'crews', 'can', 'not', 'reach', '.']\"\n",
            " b\"['Mr.', 'Egeland', 'said', 'the', 'latest', 'figures', 'show', '1.8', 'million', 'people', 'are', 'in', 'need', 'of', 'food', 'assistance', '-', 'with', 'the', 'need', 'greatest', 'in', 'Indonesia', ',', 'Sri', 'Lanka', ',', 'the', 'Maldives', 'and', 'India', '.']\"\n",
            " b'[\\'He\\', \\'said\\', \\'last\\', \\'week\\', \"\\'s\", \\'tsunami\\', \\'and\\', \\'the\\', \\'massive\\', \\'underwater\\', \\'earthquake\\', \\'that\\', \\'triggered\\', \\'it\\', \\'has\\', \\'affected\\', \\'millions\\', \\'in\\', \\'Asia\\', \\'and\\', \\'Africa\\', \\'.\\']'\n",
            " b\"['Some', '1,27,000', 'people', 'are', 'known', 'dead', '.']\"\n",
            " b\"['Aid', 'is', 'being', 'rushed', 'to', 'the', 'region', ',', 'but', 'the', 'U.N.', 'official', 'stressed', 'that', 'bottlenecks', 'and', 'a', 'lack', 'of', 'infrastructure', 'remain', 'a', 'challenge', '.']\"\n",
            " b'[\\'Lebanese\\', \\'politicians\\', \\'are\\', \\'condemning\\', \\'Friday\\', \"\\'s\", \\'bomb\\', \\'blast\\', \\'in\\', \\'a\\', \\'Christian\\', \\'neighborhood\\', \\'of\\', \\'Beirut\\', \\'as\\', \\'an\\', \\'attempt\\', \\'to\\', \\'sow\\', \\'sectarian\\', \\'strife\\', \\'in\\', \\'the\\', \\'formerly\\', \\'war-torn\\', \\'country\\', \\'.\\']'\n",
            " b\"['In', 'Beirut', ',', 'a', 'string', 'of', 'officials', 'voiced', 'their', 'anger', ',', 'while', 'at', 'the', 'United', 'Nations', 'summit', 'in', 'New', 'York', ',', 'Prime', 'Minister', 'Fouad', 'Siniora', 'said', 'the', 'Lebanese', 'people', 'are', 'resolute', 'in', 'preventing', 'such', 'attempts', 'from', 'destroying', 'their', 'spirit', '.']\"\n",
            " b\"['One', 'person', 'was', 'killed', 'and', 'more', 'than', '20', 'others', 'injured', 'in', 'the', 'bomb', 'blast', 'late', 'Friday', ',', 'which', 'took', 'place', 'on', 'a', 'residential', 'street', '.']\"\n",
            " b\"['Lebanon', 'has', 'suffered', 'a', 'series', 'of', 'bombings', 'since', 'the', 'massive', 'explosion', 'in', 'February', 'that', 'killed', 'former', 'Prime', 'Minister', 'Rafik', 'Hariri', 'and', '20', 'other', 'people', '.']\"\n",
            " b'[\\'Syria\\', \\'is\\', \\'widely\\', \\'accused\\', \\'of\\', \\'involvement\\', \\'in\\', \\'his\\', \\'killing\\', \\',\\', \\'and\\', \\'Friday\\', \"\\'s\", \\'explosion\\', \\'comes\\', \\'days\\', \\'before\\', \\'U.N.\\', \\'investigator\\', \\'Detlev\\', \\'Mehlis\\', \\'is\\', \\'to\\', \\'return\\', \\'to\\', \\'Damascus\\', \\'to\\', \\'interview\\', \\'several\\', \\'Syrian\\', \\'officials\\', \\'about\\', \\'the\\', \\'assassination\\', \\'.\\']'\n",
            " b'[\\'The\\', \\'global\\', \\'financial\\', \\'crisis\\', \\'has\\', \\'left\\', \\'Iceland\\', \"\\'s\", \\'economy\\', \\'in\\', \\'shambles\\', \\'.\\']'\n",
            " b\"['Israeli', 'officials', 'say', 'Prime', 'Minister', 'Ariel', 'Sharon', 'will', 'undergo', 'a', 'medical', 'procedure', 'Thursday', 'to', 'close', 'a', 'tiny', 'hole', 'in', 'his', 'heart', 'discovered', 'during', 'treatment', 'for', 'a', 'minor', 'stroke', 'suffered', 'last', 'month', '.']\"\n",
            " b'[\\'Doctors\\', \\'describe\\', \\'the\\', \\'tiny\\', \\'hole\\', \\'as\\', \\'a\\', \\'minor\\', \\'birth\\', \\'defect\\', \\'and\\', \\'say\\', \\'it\\', \\'is\\', \\'in\\', \\'the\\', \\'partition\\', \\'between\\', \\'the\\', \\'upper\\', \\'chambers\\', \\'of\\', \\'Mr.\\', \\'Sharon\\', \"\\'s\", \\'heart\\', \\'.\\']'\n",
            " b\"['The', 'procedure', ',', 'known', 'as', 'cardiac', 'catheterization', ',', 'involves', 'inserting', 'a', 'catheter', 'through', 'a', 'blood', 'vessel', 'into', 'the', 'heart', ',', 'where', 'an', 'umbrella-like', 'device', 'will', 'plug', 'the', 'hole', '.']\"\n",
            " b\"['Doctors', 'say', 'they', 'expect', 'Mr.', 'Sharon', 'will', 'make', 'a', 'full', 'recovery', '.']\"\n",
            " b\"['Mr.', 'Sharon', 'returned', 'to', 'work', 'on', 'December', '25', ',', 'one', 'week', 'after', 'his', 'emergency', 'hospitalization', '.']\"\n",
            " b\"['Doctors', 'say', 'the', 'stroke', 'has', 'not', 'caused', 'any', 'permanent', 'damage', '.']\"\n",
            " b\"['The', 'designers', 'of', 'the', 'first', 'private', 'manned', 'rocket', 'to', 'burst', 'into', 'space', 'have', 'received', 'a', '$', '10', 'million', 'prize', 'created', 'to', 'promote', 'space', 'tourism', '.']\"\n",
            " b\"['SpaceShipOne', 'designer', 'Burt', 'Rutan', 'accepted', 'the', 'Ansari', 'X', 'Prize', 'money', 'and', 'a', 'trophy', 'on', 'behalf', 'of', 'his', 'team', 'Saturday', 'during', 'an', 'awards', 'ceremony', 'in', 'the', 'U.S.', 'state', 'of', 'Missouri', '.']\"\n",
            " b\"['To', 'win', 'the', 'money', ',', 'SpaceShipOne', 'had', 'to', 'blast', 'off', 'into', 'space', 'twice', 'in', 'a', 'two-week', 'period', 'and', 'fly', 'at', 'least', '100', 'kilometers', 'above', 'Earth', '.']\"\n",
            " b'[\\'The\\', \\'spacecraft\\', \\'made\\', \\'its\\', \\'flights\\', \\'in\\', \\'late\\', \\'September\\', \\'and\\', \\'early\\', \\'October\\', \\',\\', \\'lifting\\', \\'off\\', \\'from\\', \\'California\\', \"\\'s\", \\'Mojave\\', \\'desert\\', \\'.\\']'\n",
            " b\"['Three', 'major', 'banks', 'have', 'collapsed', ',', 'unemployment', 'has', 'soared', ',', 'and', 'the', 'value', 'of', 'the', 'krona', 'has', 'plunged', '.']\"\n",
            " b\"['The', 'vehicle', 'had', 'to', 'carry', 'a', 'pilot', 'and', 'weight', 'equivalent', 'to', 'two', 'passengers', '.']\"\n",
            " b\"['SpaceShipOne', 'was', 'financed', 'with', 'more', 'than', '$', '20', 'million', 'from', 'Paul', 'Allen', ',', 'a', 'co-founder', 'of', 'the', 'Microsoft', 'Corporation', '.']\"\n",
            " b'[\\'North\\', \\'Korea\\', \\'says\\', \\'flooding\\', \\'caused\\', \\'by\\', \\'last\\', \\'week\\', \"\\'s\", \\'typhoon\\', \\',\\', \\'Wipha\\', \\',\\', \\'has\\', \\'destroyed\\', \\'14,000\\', \\'homes\\', \\'and\\', \\'1,09,000\\', \\'hectares\\', \\'of\\', \\'crops\\', \\'.\\']'\n",
            " b\"['The', 'state', 'news', 'agency', 'KCNA', 'reported', 'the', 'damage', 'Monday', '.']\"\n",
            " b\"['It', 'says', 'the', 'floods', 'also', 'destroyed', 'or', 'damaged', '8,000', 'public', 'buildings', 'and', 'washed', 'out', 'roads', ',', 'bridges', 'and', 'railways', '.']\"\n",
            " b\"['The', 'report', 'did', 'not', 'mention', 'any', 'deaths', 'or', 'injuries', '.']\"]\n",
            "Padded NER tag IDs: [[16 16 16 ...  0  0  0]\n",
            " [ 3 16 16 ...  0  0  0]\n",
            " [16 16  7 ...  0  0  0]\n",
            " ...\n",
            " [16 16 16 ...  0  0  0]\n",
            " [16 16 16 ...  0  0  0]\n",
            " [16 16 16 ...  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcrf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2DDcPjRRofd",
        "outputId": "bda5f39b-cd3e-4e9c-a03b-45861c33a9fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchcrf in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchcrf) (2.0.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from torchcrf) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->torchcrf) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->torchcrf) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show torchcrf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlOUgrU2TO5V",
        "outputId": "36377238-d43c-41bf-8c1f-86e62faaadfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: TorchCRF\n",
            "Version: 1.1.0\n",
            "Summary: An Implementation of Conditional Random Fields in pytorch\n",
            "Home-page: https://github.com/s14t284/TorchCRF\n",
            "Author: Ryuya Ikeda\n",
            "Author-email: rikeda71@gmail.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: numpy, torch\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchcrf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "gu-haCKHSmp2",
        "outputId": "8907f845-99e9-4da0-d933-6a70be78df25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchcrf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a996d622c3b5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchcrf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchcrf'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchcrf import CRF\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# ---------------\n",
        "# Model definitions\n",
        "# ---------------\n",
        "\n",
        "class BiLSTM_CRF_WithChar(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, char_vocab_size,\n",
        "                 word_embedding_dim=100, char_embedding_dim=30, char_out_channels=50, hidden_dim=256):\n",
        "        super(BiLSTM_CRF_WithChar, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_embedding_dim)\n",
        "        self.char_cnn = CharCNN(char_vocab_size, char_embedding_dim, char_out_channels, kernel_size=3)\n",
        "        # Concatenate word and character features.\n",
        "        combined_dim = word_embedding_dim + char_out_channels\n",
        "        self.lstm = nn.LSTM(combined_dim, hidden_dim // 2, num_layers=1,\n",
        "                            bidirectional=True, batch_first=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "        self.crf = CRF(tagset_size, batch_first=True)\n",
        "\n",
        "    def forward(self, sentences, words_chars, tags=None):\n",
        "        \"\"\"\n",
        "        sentences: Tensor of shape (batch_size, seq_len) containing word indices.\n",
        "        words_chars: Tensor of shape (batch_size, seq_len, word_length) containing character indices.\n",
        "        tags: (Optional) Tensor of shape (batch_size, seq_len) containing tag indices.\n",
        "        \"\"\"\n",
        "        word_embeds = self.word_embeddings(sentences)  # (batch_size, seq_len, word_embedding_dim)\n",
        "        char_embeds = self.char_cnn(words_chars)         # (batch_size, seq_len, char_out_channels)\n",
        "        # Concatenate along the feature dimension.\n",
        "        embeds = torch.cat([word_embeds, char_embeds], dim=2)  # (batch_size, seq_len, combined_dim)\n",
        "        lstm_out, _ = self.lstm(embeds)                 # (batch_size, seq_len, hidden_dim)\n",
        "        emissions = self.hidden2tag(lstm_out)           # (batch_size, seq_len, tagset_size)\n",
        "\n",
        "        if tags is not None:\n",
        "            loss = -self.crf(emissions, tags, mask=(sentences != 0))\n",
        "            return loss\n",
        "        else:\n",
        "            prediction = self.crf.decode(emissions)\n",
        "            return prediction\n",
        "\n",
        "class CharCNN(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_embedding_dim=30, out_channels=50, kernel_size=3):\n",
        "        super(CharCNN, self).__init__()\n",
        "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
        "        self.conv1d = nn.Conv1d(in_channels=char_embedding_dim,\n",
        "                                out_channels=out_channels, kernel_size=kernel_size)\n",
        "\n",
        "    def forward(self, words_chars):\n",
        "        \"\"\"\n",
        "        words_chars: Tensor of shape (batch_size, seq_len, word_length)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, word_length = words_chars.size()\n",
        "        # Reshape to (batch_size*seq_len, word_length)\n",
        "        words_chars = words_chars.view(-1, word_length)\n",
        "        emb = self.char_embedding(words_chars)  # (batch_size*seq_len, word_length, char_embedding_dim)\n",
        "        # Permute for convolution: (batch_size*seq_len, char_embedding_dim, word_length)\n",
        "        emb = emb.permute(0, 2, 1)\n",
        "        conv_out = self.conv1d(emb)  # (batch_size*seq_len, out_channels, L_out)\n",
        "        # Apply max pooling over time dimension (L_out) to get fixed-length feature per word.\n",
        "        char_features = torch.max(conv_out, dim=2)[0]  # (batch_size*seq_len, out_channels)\n",
        "        # Reshape back to (batch_size, seq_len, out_channels)\n",
        "        char_features = char_features.view(batch_size, seq_len, -1)\n",
        "        return char_features\n",
        "\n",
        "# ---------------\n",
        "# Data Processing Utilities\n",
        "# ---------------\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return text.strip().split()\n",
        "\n",
        "def build_vocab(sequences, min_freq=1, special_tokens=['<PAD>', '<UNK>']):\n",
        "    counter = Counter()\n",
        "    for seq in sequences:\n",
        "        counter.update(seq)\n",
        "    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
        "    for word, count in counter.items():\n",
        "        if count >= min_freq and word not in vocab:\n",
        "            vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "def encode_sequence(seq, vocab):\n",
        "    return [vocab.get(token, vocab['<UNK>']) for token in seq]\n",
        "\n",
        "def pad_sequences(sequences, pad_value, max_len=None):\n",
        "    if max_len is None:\n",
        "        max_len = max(len(seq) for seq in sequences)\n",
        "    padded = []\n",
        "    for seq in sequences:\n",
        "        seq = seq[:max_len]\n",
        "        padded_seq = seq + [pad_value] * (max_len - len(seq))\n",
        "        padded.append(padded_seq)\n",
        "    return padded, max_len\n",
        "\n",
        "# For characters, pad each word to a fixed length.\n",
        "def pad_word(word_seq, max_word_len):\n",
        "    if len(word_seq) >= max_word_len:\n",
        "        return word_seq[:max_word_len]\n",
        "    else:\n",
        "        return word_seq + [0]*(max_word_len - len(word_seq))\n",
        "\n",
        "# ---------------\n",
        "# PyTorch Dataset Definition\n",
        "# ---------------\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, df, word_vocab, tag_vocab, char_vocab, max_word_len=10, sentence_max_len=None):\n",
        "        \"\"\"\n",
        "        df: DataFrame with 'sentence' and 'tags' columns (whitespace separated strings)\n",
        "        word_vocab: mapping from words to indices.\n",
        "        tag_vocab: mapping from tags to indices.\n",
        "        char_vocab: mapping from characters to indices.\n",
        "        max_word_len: maximum number of characters per word.\n",
        "        sentence_max_len: optional fixed sentence length; if None, use the longest sentence in the data.\n",
        "        \"\"\"\n",
        "        self.sentences = []\n",
        "        self.tags = []\n",
        "        self.chars = []\n",
        "        self.max_word_len = max_word_len\n",
        "\n",
        "        # Tokenize sentences and tags.\n",
        "        for _, row in df.iterrows():\n",
        "            word_tokens = tokenize_text(row['sentence'])\n",
        "            tag_tokens = tokenize_text(row['tags'])\n",
        "            # Skip if number of tokens and tags do not match.\n",
        "            if len(word_tokens) != len(tag_tokens):\n",
        "                continue\n",
        "            self.sentences.append(word_tokens)\n",
        "            self.tags.append(tag_tokens)\n",
        "\n",
        "            # Build character sequences per word.\n",
        "            char_seq = []\n",
        "            for word in word_tokens:\n",
        "                chars = [char_vocab.get(c, 0) for c in list(word)]\n",
        "                # Pad individual word to fixed length.\n",
        "                chars = pad_word(chars, max_word_len)\n",
        "                char_seq.append(chars)\n",
        "            self.chars.append(char_seq)\n",
        "\n",
        "        # Optionally fix the sentence length\n",
        "        if sentence_max_len is None:\n",
        "            self.sentence_max_len = max(len(s) for s in self.sentences)\n",
        "        else:\n",
        "            self.sentence_max_len = sentence_max_len\n",
        "\n",
        "        # Encode sequences\n",
        "        self.sentences = [encode_sequence(s, word_vocab) for s in self.sentences]\n",
        "        self.tags = [encode_sequence(s, tag_vocab) for s in self.tags]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tags = self.tags[idx]\n",
        "        chars = self.chars[idx]\n",
        "        return {'words': torch.tensor(sent, dtype=torch.long),\n",
        "                'tags': torch.tensor(tags, dtype=torch.long),\n",
        "                'chars': torch.tensor(chars, dtype=torch.long)}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Pads the sequences in a batch to the maximum length found in that batch.\n",
        "    \"\"\"\n",
        "    words = [item['words'] for item in batch]\n",
        "    tags = [item['tags'] for item in batch]\n",
        "    chars = [item['chars'] for item in batch]\n",
        "\n",
        "    # Pad word sequences and tag sequences.\n",
        "    words_padded, max_len = pad_sequences([w.tolist() for w in words], pad_value=0)\n",
        "    tags_padded, _ = pad_sequences([t.tolist() for t in tags], pad_value=0)\n",
        "\n",
        "    # For characters, pad on sentence dimension.\n",
        "    padded_chars = []\n",
        "    for char_seq in chars:\n",
        "        # Pad sentence length for characters.\n",
        "        padded_sentence = char_seq.tolist() + [[0]*char_seq.size(1)] * (max_len - char_seq.size(0))\n",
        "        padded_chars.append(padded_sentence)\n",
        "\n",
        "    words_tensor = torch.tensor(words_padded, dtype=torch.long)\n",
        "    tags_tensor = torch.tensor(tags_padded, dtype=torch.long)\n",
        "    chars_tensor = torch.tensor(padded_chars, dtype=torch.long)\n",
        "\n",
        "    return words_tensor, chars_tensor, tags_tensor\n",
        "\n",
        "# ---------------\n",
        "# Main Training and Evaluation Code\n",
        "# ---------------\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Import dataset from CSV and split manually\n",
        "    folder_path = \"/content/drive/MyDrive/comp-4211-spring-25-project/\"\n",
        "    df = pd.read_csv(os.path.join(folder_path, \"train.csv\"))\n",
        "    train_df = df[:32000]\n",
        "    test_df = df[32000:]\n",
        "\n",
        "    # Prepare token lists from training data (you could also include test for a larger vocab)\n",
        "    word_sequences = [tokenize_text(s) for s in train_df['sentence']]\n",
        "    tag_sequences = [tokenize_text(t) for t in train_df['tags']]\n",
        "    char_sequences = [[list(word) for word in s] for s in word_sequences]\n",
        "\n",
        "    # Build vocabularies for words, tags, and characters.\n",
        "    word_vocab = build_vocab(word_sequences, special_tokens=['<PAD>', '<UNK>'])\n",
        "    tag_set = sorted(set(token for tags in tag_sequences for token in tags))\n",
        "    # For tag vocab, we include padding as tag index 0.\n",
        "    tag_vocab = {tag: idx + 1 for idx, tag in enumerate(tag_set)}\n",
        "    tag_vocab['<PAD>'] = 0\n",
        "    # For characters, include padding token (index 0).\n",
        "    char_vocab = build_vocab([list(word) for sent in word_sequences for word in sent], special_tokens=['<PAD>', '<UNK>'])\n",
        "\n",
        "    print(\"Word vocab size:\", len(word_vocab))\n",
        "    print(\"Tag vocab size:\", len(tag_vocab))\n",
        "    print(\"Char vocab size:\", len(char_vocab))\n",
        "\n",
        "    # Create Dataset and DataLoader for training and test.\n",
        "    max_word_len = 10  # you can adjust this value\n",
        "    train_dataset = NERDataset(train_df, word_vocab, tag_vocab, char_vocab, max_word_len=max_word_len)\n",
        "    test_dataset = NERDataset(test_df, word_vocab, tag_vocab, char_vocab, max_word_len=max_word_len,\n",
        "                              sentence_max_len=train_dataset.sentence_max_len)\n",
        "\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Model initialization.\n",
        "    vocab_size = len(word_vocab)\n",
        "    tagset_size = len(tag_vocab)\n",
        "    char_vocab_size = len(char_vocab)\n",
        "\n",
        "    model = BiLSTM_CRF_WithChar(vocab_size, tagset_size, char_vocab_size).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    num_epochs = 10  # adjust the number of epochs as necessary\n",
        "\n",
        "    # Training loop.\n",
        "    model.train()\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            words, chars, tags = [b.to(device) for b in batch]\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(words, chars, tags)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Evaluation on test set\n",
        "    model.eval()\n",
        "    predictions_all = []\n",
        "    tags_all = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            words, chars, tags = [b.to(device) for b in batch]\n",
        "            predictions = model(words, chars)\n",
        "            predictions_all.extend(predictions)\n",
        "            tags_all.extend(tags.cpu().tolist())\n",
        "\n",
        "    # (Optional) Post-process predictions (e.g., convert tag indices back to tag strings)\n",
        "    idx2tag = {idx: tag for tag, idx in tag_vocab.items()}\n",
        "    predictions_str = []\n",
        "    for sent_pred in predictions_all:\n",
        "        sent_tags = [idx2tag.get(idx, \"<UNK>\") for idx in sent_pred]\n",
        "        predictions_str.append(sent_tags)\n",
        "    # Here you could compute evaluation metrics if you have the gold labels.\n",
        "\n",
        "    # Example: print first few predictions from test set\n",
        "    for i in range(5):\n",
        "        print(f\"Sentence {i+1} prediction:\", predictions_str[i])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "2Y7V2RXNPzKQ",
        "outputId": "0e19320b-412f-4d27-af93-38c0cba29015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchcrf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8c413e32118e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchcrf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCRF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchcrf'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eYdF1DgYPzH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KKam7mujPzFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G5PkCkrIPzDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tWAVT15cPzAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}